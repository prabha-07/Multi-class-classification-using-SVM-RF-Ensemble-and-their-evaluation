{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "*AI Aid: ChatGPT was used by me to get assistance with this homework"
   ],
   "metadata": {
    "id": "ZeEBkQSHCPIb"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K3YKXGpeebtK"
   },
   "outputs": [],
   "source": [
    "import warnings, os, math\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error, ConfusionMatrixDisplay\n",
    "from sklearn.base import clone\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.feature_selection import f_classif"
   ],
   "metadata": {
    "id": "1z7ZoC_4fVLL"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df = pd.read_excel('/content/drive/MyDrive/Dry_Bean_Dataset.xlsx')\n",
    "df.head(5)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "id": "WGprJWO1fW2K",
    "outputId": "64e8a3bc-8a63-4e4f-dd6f-73385b4c4eba"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Q1\n",
    "1. Below displayed the statistical measures for the attributes along with histogram plots\n",
    "2. Most of the attributes show no clear evidence of skewness or any irregularities in the data. There aren't any null values in the dataset as well.\n",
    "3. Since target lablels are texts, I've label encoded them to numerical values\n",
    "4. I've split the dataset into Train - 60%, Val - 20% and Test - 20%"
   ],
   "metadata": {
    "id": "zcUFyZW3042R"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "df.describe().T"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "id": "kRSUwB7-gEyT",
    "outputId": "aa10f473-771c-4ac9-8f9f-fb50f25bff45"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df.isnull().sum()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 610
    },
    "id": "m7zhDo2sgPYp",
    "outputId": "2f2ce3bc-3d09-4dfe-9133-f1832ee139ad"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df.hist(bins=20, figsize=(15, 10))\n",
    "plt.show()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 853
    },
    "id": "eIVmgyckgULb",
    "outputId": "ae90707e-9afa-4699-9d51-8521cc4897eb"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "X = df.drop(columns=['Class'])\n",
    "y = df['Class']"
   ],
   "metadata": {
    "id": "zmHnU5H7gfdS"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# First split into train and temp (80% train, 20% temp)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.4, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Then split the temp set equally into validation and test (20% each)\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(\"Train shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Validation shape:\", X_val.shape, y_val.shape)\n",
    "print(\"Test shape:\", X_test.shape, y_test.shape)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TW5IlRKczZHz",
    "outputId": "85d131f7-bb8f-48ef-80de-c8b5745aa8fe"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ],
   "metadata": {
    "id": "tFl-5dK20HgA"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_train)  # fit only on training labels\n",
    "\n",
    "y_train_enc = encoder.transform(y_train)\n",
    "y_val_enc   = encoder.transform(y_val)\n",
    "y_test_enc  = encoder.transform(y_test)\n",
    "\n",
    "print(dict(zip(encoder.classes_, encoder.transform(encoder.classes_))))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ruwmtlf-1ro3",
    "outputId": "8d9233ad-48a5-4f10-fa63-22aeb17e0433"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Q2\n",
    "1. Below I've plotted Pearson correlation matrix for between features and we can see set of features are highly correlated between them and negatively correlated with other set of features, which shows clear identifying character for their classes.\n",
    "2. For feature and label relationship, I've performed ANOVA F-test, which gave higher value F score for many features which also shows the strong character for identifying their classes. Also, I've made scatter plot between features and label.\n",
    "3. Also feature importance plot shows the top features to contribute in identifying their classes such as area, perimeter, e.t.c"
   ],
   "metadata": {
    "id": "9_3KGSiJ2BJo"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "X = X_train\n",
    "y = y_train_enc\n",
    "\n",
    "# Feature-Feature Correlation Heatmap\n",
    "corr_matrix = X.corr(method='pearson')\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm',\n",
    "            center=0, square=True, linewidths=0.5)\n",
    "plt.title('Pearson Correlation Matrix - Bean', fontsize=16, pad=20)\n",
    "plt.tight_layout()\n",
    "plt.savefig('correlation_heatmap.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# Feature-Label Relationship (ANOVA F-test)\n",
    "f_scores, p_values = f_classif(X, y)\n",
    "\n",
    "feature_label_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'ANOVA_F': f_scores,\n",
    "    'P-value': p_values\n",
    "})\n",
    "feature_label_df['Significant'] = np.where(feature_label_df['P-value'] < 0.05, 'Yes', 'No')\n",
    "feature_label_df = feature_label_df.sort_values('ANOVA_F', ascending=False)\n",
    "\n",
    "print(\"\\n=== Feature-Label Association (ANOVA F-test) ===\")\n",
    "print(feature_label_df[['Feature', 'ANOVA_F', 'P-value', 'Significant']].to_string(index=False))\n",
    "\n",
    "# Feature-Label Bar Plot (F-scores)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_label_df['Feature'], feature_label_df['ANOVA_F'], color='skyblue', alpha=0.8)\n",
    "plt.xlabel('ANOVA F-score')\n",
    "plt.title('Feature Importance based on F-test')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_label_importance.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# Features vs Label Scatter Plots (Class-colored)\n",
    "fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n",
    "axes = axes.flatten()\n",
    "colors = ['red', 'green', 'blue']\n",
    "\n",
    "for i, feature in enumerate(X.columns):\n",
    "    if i >= len(axes):\n",
    "        break\n",
    "    ax = axes[i]\n",
    "    scatter = ax.scatter(X[feature], y, c=y, cmap=plt.cm.Set1, alpha=0.6)\n",
    "    ax.set_xlabel(feature, fontsize=9)\n",
    "    ax.set_ylabel('Label', fontsize=9)\n",
    "    ax.set_title(f'{feature}', fontsize=9)\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "# Remove empty subplots\n",
    "if len(X.columns) < len(axes):\n",
    "    for j in range(len(X.columns), len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "plt.suptitle('Features vs Label (Class-colored)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig('scatter_features_vs_label.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Analysis complete! Check the saved PNG files.\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "hdk51rut0at8",
    "outputId": "9e7e5911-0a42-4be3-ef8b-9f07cf161e21"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def show_confusion(y_true, y_pred, title):\n",
    "    disp = ConfusionMatrixDisplay.from_predictions(y_true, y_pred)\n",
    "    disp.ax_.set_title(title)\n",
    "    plt.show()"
   ],
   "metadata": {
    "id": "LsxVyl5TpT3F"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Q3\n",
    "It was valid for question 1 to split the dataset. I've already mentioned about the split of dataset for this question"
   ],
   "metadata": {
    "id": "u1kQoTRD4RTt"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Q4\n",
    "1. I've trained the classifiers: Softmax, SVM and RandomForest and tweaked their hyperparameters for the best fit model. Results are down the line.\n",
    "2. I've reported classification metrics under each model, as well as confusion matrix and classification report of 3 best classifiers down the line.\n",
    "3. Impact of hyperparameters has been discussed under each model below."
   ],
   "metadata": {
    "id": "gvRwi3pw4oJ7"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Softmax Regression"
   ],
   "metadata": {
    "id": "E6qztRxn8z0C"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import ParameterGrid"
   ],
   "metadata": {
    "id": "CyGb_1yx8wRM"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10],\n",
    "    'solver': ['lbfgs', 'saga'],\n",
    "    'max_iter': [100, 200, 500, 700, 1000]\n",
    "}\n",
    "\n",
    "best_f1_lr = -1\n",
    "best_params_lr = None\n",
    "best_model_lr = None\n",
    "\n",
    "# Manual search — train on training set, evaluate on validation set\n",
    "for params in ParameterGrid(param_grid):\n",
    "    model = LogisticRegression(**params, multi_class='multinomial')\n",
    "    model.fit(X_train_scaled, y_train_enc)\n",
    "\n",
    "    y_val_pred = model.predict(X_val_scaled)\n",
    "    f1 = f1_score(y_val_enc, y_val_pred, average='weighted')  # 'weighted' handles class imbalance\n",
    "\n",
    "    if f1 > best_f1_lr:\n",
    "        best_f1_lr = f1\n",
    "        best_params_lr = params\n",
    "        best_model_lr = model\n",
    "\n",
    "print(\"Best Logistic Regression Params:\", best_params_lr)\n",
    "print(\"Best Validation F1-Score:\", best_f1_lr)"
   ],
   "metadata": {
    "id": "-owW7g0885l0",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "058f9313-d825-48f4-cf0a-ede503944927"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Impact of hyperparameters\n",
    "1. C: higher C values (like 1 or 10) likely gave higher F1-scores — the model became more flexible and better fit the training data, while smaller C (e.g., 0.01) underfit and produced lower validation F1.\n",
    "2. solver: lbfgs probably converged faster and gave stable scores, while saga might have been slower or slightly noisier in F1 due to stochastic updates.\n",
    "3. max_iter: The model didn't show any better or worse results when it was increased more than 100\n",
    "4. the best-performing combination balanced a moderate–high C, used lbfgs, and had enough max_iter to fully converge, resulting in the highest validation F1-score printed at the end."
   ],
   "metadata": {
    "id": "gAjO7nNS6kG1"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def evaluate_model(model, X_train, y_train, X_val, y_val, X_test, y_test):\n",
    "    results = {}\n",
    "    for name, X, y in [('Train', X_train, y_train), ('Validation', X_val, y_val), ('Test', X_test, y_test)]:\n",
    "        y_pred = model.predict(X)\n",
    "        results[name] = {\n",
    "            'Accuracy': accuracy_score(y, y_pred),\n",
    "            'Precision': precision_score(y, y_pred, average='weighted'),\n",
    "            'Recall': recall_score(y, y_pred, average='weighted'),\n",
    "            'F1': f1_score(y, y_pred, average='weighted')\n",
    "        }\n",
    "    return pd.DataFrame(results).T"
   ],
   "metadata": {
    "id": "ADY70rsQEp-P"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "lr_results = evaluate_model(best_model_lr, X_train_scaled, y_train_enc, X_val_scaled, y_val_enc, X_test_scaled, y_test_enc)\n",
    "print(lr_results)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dsx9HFUiE9tm",
    "outputId": "83b9cd2c-31ab-49f2-b543-3cf7775aea88"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Support Vector Machine"
   ],
   "metadata": {
    "id": "K5p_sjpcFdJ2"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'C': [0.01,0.1, 1, 10],\n",
    "    'kernel': ['linear', 'poly', 'rbf'],\n",
    "    'degree': [2, 3],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "best_f1_svm = -1\n",
    "best_params_svm = None\n",
    "best_model_svm = None\n",
    "\n",
    "# Manual search — train on training set, evaluate on validation set\n",
    "for params in ParameterGrid(param_grid):\n",
    "    model = SVC(**params,probability=True)\n",
    "    model.fit(X_train_scaled, y_train_enc)\n",
    "\n",
    "    y_val_pred = model.predict(X_val_scaled)\n",
    "    f1 = f1_score(y_val_enc, y_val_pred, average='weighted')  # 'weighted' handles class imbalance\n",
    "\n",
    "    if f1 > best_f1_svm:\n",
    "        best_f1_svm = f1\n",
    "        best_params_svm = params\n",
    "        best_model_svm = model\n",
    "\n",
    "print(\"Best Logistic Regression Params:\", best_params_svm)\n",
    "print(\"Best Validation F1-Score:\", best_f1_svm)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K_QTWdYcFjQN",
    "outputId": "1e25eb0a-f63b-4644-d1e8-aaaa8679d08d"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Impact of Hyperparameters\n",
    "1. C - Higher C (like 1 or 10) likely improved F1 on the validation set by letting the model fit harder boundaries, but slightly risking overfitting. Lower C (like 0.01) underfit and gave weaker separation between classes.\n",
    "2. kernel - The rbf kernel probably yielded the best F1 since it captures non-linear relationships\n",
    "3. degree - This mattered only for the poly kernel; Hence for rbf and linear it had no effect.\n",
    "4. gamma - With the rbf kernel, 'scale' generally gave better F1 than 'auto', since it adapts gamma based on feature variance.\n",
    "5. Best SVM likely used a moderate–high C, the rbf kernel with gamma='scale', and a higher iteration kernel fit — giving the balance between fitting complex patterns and keeping validation F1 stable."
   ],
   "metadata": {
    "id": "xHsevqNj7EuG"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "svm_results = evaluate_model(best_model_svm, X_train_scaled, y_train_enc, X_val_scaled, y_val_enc, X_test_scaled, y_test_enc)\n",
    "print(\"\\n=== SVM Performance ===\")\n",
    "print(svm_results)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tSpPkarGJGfL",
    "outputId": "59720cd1-59e2-446a-e54d-c9109b67d757"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Random Forest Classifier"
   ],
   "metadata": {
    "id": "Xycw7-ScJ1IF"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "best_f1_rf = -1\n",
    "best_params_rf = None\n",
    "best_model_rf = None\n",
    "\n",
    "# Manual search — train on training set, evaluate on validation set\n",
    "for params in ParameterGrid(param_grid):\n",
    "    model = RandomForestClassifier(**params,random_state=42)\n",
    "    model.fit(X_train_scaled, y_train_enc)\n",
    "\n",
    "    y_val_pred = model.predict(X_val_scaled)\n",
    "    f1 = f1_score(y_val_enc, y_val_pred, average='weighted')  # 'weighted' handles class imbalance\n",
    "\n",
    "    if f1 > best_f1_rf:\n",
    "        best_f1_rf = f1\n",
    "        best_params_rf = params\n",
    "        best_model_rf = model\n",
    "\n",
    "print(\"Best Logistic Regression Params:\", best_params_rf)\n",
    "print(\"Best Validation F1-Score:\", best_f1_rf)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mM0zhaFQJ-au",
    "outputId": "094d6333-076e-43cb-9cc3-52ce4c40d933"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Impact of hyperparameters\n",
    "1. n_estimators: More trees (200 vs 100) slightly improved validation F1 by stabilizing predictions.\n",
    "2. max_depth: Although max_depth was not limited, restricting other hyperparameters ensured best results while not overfitting much.\n",
    "3. min_samples_split and min_samples_leaf: Inspite of lower values, since validation F-1 was not significant with respect to train F-1, overfitting did not seem to be a concern"
   ],
   "metadata": {
    "id": "iI19F1h28IlV"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "rf_results = evaluate_model(best_model_rf, X_train_scaled, y_train_enc, X_val_scaled, y_val_enc, X_test_scaled, y_test_enc)\n",
    "print(\"\\n=== SVM Performance ===\")\n",
    "print(svm_results)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bDD7nqH7M7RJ",
    "outputId": "9e69588e-8a65-4099-9d79-b6ca2cc8d3b2"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# RandomForest feature importance\n",
    "imp = pd.Series(best_model_rf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "display(imp.to_frame(\"RF_feature_importance\"))\n",
    "imp.plot(kind=\"bar\", figsize=(10,4), title=\"RandomForest Feature Importance\"); plt.show()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "tN3FNxvKt7-5",
    "outputId": "dba5e9ea-b350-4662-91fd-b4270a0c8bf8"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Classification report and confusion matrix for Train, Val and Test of best models from each classifier"
   ],
   "metadata": {
    "id": "nXMCjbjwwF5v"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Evaluate best models on train/val/test\n",
    "def eval_and_show(model, name):\n",
    "    print(f\"\\n=== {name}===\")\n",
    "    for split_name, (X_, y_) in {\n",
    "        \"TRAIN\": (X_train_scaled,y_train_enc),\n",
    "        \"VAL\"  : (X_val_scaled,y_val_enc),\n",
    "        \"TEST\" : (X_test_scaled,y_test_enc)\n",
    "    }.items():\n",
    "        yp = model.predict(X_)\n",
    "        print(f\"\\n{split_name} report\")\n",
    "        print(classification_report(y_, yp, digits=3))\n",
    "        show_confusion(y_, yp, f\"{name} — {split_name}\")\n",
    "\n",
    "eval_and_show(best_model_lr,  \"Best LogisticRegression\")\n",
    "eval_and_show(best_model_svm, \"Best SVM\")\n",
    "eval_and_show(best_model_rf,  \"Best RandomForest\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "SpGiQOzRwRvx",
    "outputId": "01e47003-5ab4-495f-8ca8-0a22331b3e57"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Q5\n",
    "1. I've done an ensemble of the 3 observed best classifiers and displayed the results below\n",
    "2. Both individual and ensemble gave closely similar macro and weighted average of F-1 classes on test, which affirms that the ensemle is the best and reasonable representation of the individual models for the given dataset's label prediction\n",
    "   RF avg F-1: 93%, 92%\n",
    "   SVM avg F-1: 94%, 92%\n",
    "   Softmax avg F-1: 94%, 92%\n",
    "   Ensemble avg F-1: 94%,92%"
   ],
   "metadata": {
    "id": "lawq8ysk5wm-"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Ensemble"
   ],
   "metadata": {
    "id": "vPR12D-ncz4m"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "voting_clf = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('logreg', best_model_lr),\n",
    "        ('svm', best_model_svm),\n",
    "        ('rf', best_model_rf)\n",
    "    ],\n",
    "    voting='soft',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit ensemble on the same training data\n",
    "voting_clf.fit(X_train_scaled, y_train_enc)\n",
    "\n",
    "print(\"Ensemble on validation:\")\n",
    "y_val_pred = voting_clf.predict(X_val_scaled)\n",
    "print(classification_report(y_val_enc, y_val_pred, digits=3))\n",
    "show_confusion(y_val_enc, y_val_pred, \"Ensemble — VAL\")\n",
    "\n",
    "# If ensemble wins on VAL, retrain on TRAIN+VAL, then test:\n",
    "X_trval = pd.concat([pd.DataFrame(X_train_scaled),pd.DataFrame(X_val_scaled)], axis=0).values  # convert back to numpy if needed\n",
    "y_trval = pd.concat([pd.Series(y_train_enc),pd.Series(y_val_enc)], axis=0).values\n",
    "voting_clf.fit(X_trval, y_trval)\n",
    "\n",
    "print(\"Ensemble on test (after refit on train+val):\")\n",
    "y_test_pred = voting_clf.predict(X_test_scaled)\n",
    "print(classification_report(y_test_enc, y_test_pred, digits=3))\n",
    "show_confusion(y_test_enc, y_test_pred, \"Ensemble — TEST\")"
   ],
   "metadata": {
    "id": "29DSlWyPc3b8",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "outputId": "24e5622a-861b-4e88-d2af-eb6107d3d029"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}